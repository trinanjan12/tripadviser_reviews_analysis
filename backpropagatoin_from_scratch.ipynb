{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ca489a",
   "metadata": {},
   "source": [
    "# BackPropagation using python\n",
    "**1. In this notebook we are creating a simple back propagation algorithm**\n",
    "\n",
    "**2. In backpropagation we try to learn the weights by using partial derivative(because multivariable)**\n",
    "\n",
    "**3. We nudge the weights a little bit and check if that has made the cost functoin less(convex optimization)**\n",
    "\n",
    "**4. This is done using the gradient descent**\n",
    "\n",
    "**5. Doing the minimization of the cost function we learn the weights**\n",
    "\n",
    "### References:\n",
    "1. https://cs231n.github.io/optimization-2/\n",
    "2. https://www.youtube.com/watch?v=sVjBqjDWoQA&t=3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7498242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00204c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funcitons\n",
    "\n",
    "def diff_partial(fn,idx,*args):\n",
    "    '''\n",
    "        takes the function and the index of variable as argument\n",
    "        calculates partial derivaties wrt index position\n",
    "    '''\n",
    "    delta = 0.000000000001\n",
    "    \n",
    "    y = fn(*args)\n",
    "    args = list(args)\n",
    "    args[idx] += delta\n",
    "    y1 = fn(*args)\n",
    "    \n",
    "    return (y1-y)/delta\n",
    "\n",
    "# error is y_pred - y_true \n",
    "def error(_w1,_w2,_w3,_x,_y,_z,y):\n",
    "    ypred = _w1*_x**2 + _w2*4*_y + _w3*_z\n",
    "    err = ypred-y\n",
    "    return np.sum(err*err)\n",
    "\n",
    "# performs gradient discent\n",
    "def gradient_descent(_x,_y,_z, y):\n",
    "    '''\n",
    "        input arguments : inputs to the function and the y_ture value\n",
    "        creates random weight values to start with\n",
    "        calculates gradients and nudges the weights by the gradient(initial_weight - lr*grad)\n",
    "        this is done for multiple iteration so that the error is minimized\n",
    "        returns the final weights\n",
    "    '''\n",
    "    _w1 = np.random.random()\n",
    "    _w2 = np.random.random()\n",
    "    _w3 = np.random.random()\n",
    "    learning_rate = 0.001\n",
    "    for i in range(100):\n",
    "        print(\"i: \", i, \" Error: \", error(_w1,_w2,_w3,_x,_y,_z,y))\n",
    "        dE_dw1 = diff_partial(error, 0, _w1,_w2,_w3,_x,_y,_z,y)\n",
    "        dE_dw2 = diff_partial(error, 1, _w1,_w2,_w3,_x,_y,_z,y)\n",
    "        dE_dw3 = diff_partial(error, 2, _w1,_w2,_w3,_x,_y,_z,y)\n",
    "        \n",
    "        _w1 = _w1 - learning_rate * dE_dw1\n",
    "        _w2 = _w2 - learning_rate * dE_dw2\n",
    "        _w3 = _w3 - learning_rate * dE_dw3\n",
    "    return (_w1,_w2,_w3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9824e95",
   "metadata": {},
   "source": [
    "## 1. main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0989c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the function for which we will be doing backpropagation\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{2} + 4 y + z$"
      ],
      "text/plain": [
       "x**2 + 4*y + z"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, diff\n",
    "x, y, z = symbols('x y z', real=True)\n",
    "f = x**2 + 4*y + z\n",
    "print('This is the function for which we will be doing backpropagation')\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee383324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can also write this as _w1*x**2 + _w2*4*y + _w3*z\n",
      "here _w1,_w2,_w3 are learnable parameters\n"
     ]
    }
   ],
   "source": [
    "print('we can also write this as _w1*x**2 + _w2*4*y + _w3*z')\n",
    "print('here _w1,_w2,_w3 are learnable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e9f3a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consider initial values of _w1,_w2,_w3, which is 1,1,1 according to out original funciton\n"
     ]
    }
   ],
   "source": [
    "print('consider initial values of _w1,_w2,_w3, which is 1,1,1 according to out original funciton')\n",
    "_w1,_w2,_w3 = 1,1,1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76440263",
   "metadata": {},
   "source": [
    "### eventually by doing gradient descent we want to start from a set of random values for _w1,_w2,_w3 and then reach to this initial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65a3454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate some dataset randomly by using the weights and some random values for _x,_y,_z\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print('Generate some dataset randomly by using the weights and some random values for _x,_y,_z')\n",
    "_x = np.random.random((100, 1))\n",
    "_y = np.random.random((100, 1))\n",
    "_z = np.random.random((100, 1))\n",
    "my_func = _w1*_x**2 + _w2*4*_y + _w3*_z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a683aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0  Error:  57.33034890002906\n",
      "i:  1  Error:  7.703811378819905\n",
      "i:  2  Error:  6.448902992441417\n",
      "i:  3  Error:  6.07883409369274\n",
      "i:  4  Error:  5.7437857016070355\n",
      "i:  5  Error:  5.42738859736912\n",
      "i:  6  Error:  5.128401992345568\n",
      "i:  7  Error:  4.845938569926587\n",
      "i:  8  Error:  4.5790106290179375\n",
      "i:  9  Error:  4.3267892332259175\n",
      "i:  10  Error:  4.088490529417757\n",
      "i:  11  Error:  3.8632891314152538\n",
      "i:  12  Error:  3.6504954241447454\n",
      "i:  13  Error:  3.4494139240802446\n",
      "i:  14  Error:  3.259419524014727\n",
      "i:  15  Error:  3.0798868246053317\n",
      "i:  16  Error:  2.910238528078899\n",
      "i:  17  Error:  2.7499438118460455\n",
      "i:  18  Error:  2.59848043915433\n",
      "i:  19  Error:  2.4553330338666357\n",
      "i:  20  Error:  2.3200975527433254\n",
      "i:  21  Error:  2.1923034764559945\n",
      "i:  22  Error:  2.071557069313782\n",
      "i:  23  Error:  1.9574611415121046\n",
      "i:  24  Error:  1.8496553440024166\n",
      "i:  25  Error:  1.7477829754871916\n",
      "i:  26  Error:  1.6515239764256846\n",
      "i:  27  Error:  1.5605616351358398\n",
      "i:  28  Error:  1.4746079613094372\n",
      "i:  29  Error:  1.3933921391531536\n",
      "i:  30  Error:  1.316658519252203\n",
      "i:  31  Error:  1.2441510471920658\n",
      "i:  32  Error:  1.175629920234547\n",
      "i:  33  Error:  1.1108720450057241\n",
      "i:  34  Error:  1.0496988980982271\n",
      "i:  35  Error:  0.9918928851235984\n",
      "i:  36  Error:  0.937268249307147\n",
      "i:  37  Error:  0.8856450952605649\n",
      "i:  38  Error:  0.8368708414682098\n",
      "i:  39  Error:  0.7907852333764084\n",
      "i:  40  Error:  0.7472359200300527\n",
      "i:  41  Error:  0.7060866203967698\n",
      "i:  42  Error:  0.6672037639287156\n",
      "i:  43  Error:  0.6304598493121435\n",
      "i:  44  Error:  0.595743429176888\n",
      "i:  45  Error:  0.5629372062142979\n",
      "i:  46  Error:  0.5319373159650469\n",
      "i:  47  Error:  0.5026459136504318\n",
      "i:  48  Error:  0.47496645735264964\n",
      "i:  49  Error:  0.448811362423914\n",
      "i:  50  Error:  0.424098968747613\n",
      "i:  51  Error:  0.4007455152415625\n",
      "i:  52  Error:  0.3786801271386863\n",
      "i:  53  Error:  0.35782912646674303\n",
      "i:  54  Error:  0.3381268834470629\n",
      "i:  55  Error:  0.3195106968386168\n",
      "i:  56  Error:  0.3019176698943633\n",
      "i:  57  Error:  0.28529409205763523\n",
      "i:  58  Error:  0.2695869516324061\n",
      "i:  59  Error:  0.25474541922761\n",
      "i:  60  Error:  0.24072005411221672\n",
      "i:  61  Error:  0.22746807411983586\n",
      "i:  62  Error:  0.21494457168195122\n",
      "i:  63  Error:  0.20311167918129233\n",
      "i:  64  Error:  0.1919310860519649\n",
      "i:  65  Error:  0.18136499799413125\n",
      "i:  66  Error:  0.17138157645781602\n",
      "i:  67  Error:  0.16194698005810676\n",
      "i:  68  Error:  0.15303232407899334\n",
      "i:  69  Error:  0.14460859376993618\n",
      "i:  70  Error:  0.13664810873408895\n",
      "i:  71  Error:  0.1291260083409811\n",
      "i:  72  Error:  0.12201845152750213\n",
      "i:  73  Error:  0.11530246705737869\n",
      "i:  74  Error:  0.10895594072379465\n",
      "i:  75  Error:  0.10295873374484878\n",
      "i:  76  Error:  0.09729148136667148\n",
      "i:  77  Error:  0.09193666509433815\n",
      "i:  78  Error:  0.0868768179688537\n",
      "i:  79  Error:  0.08209559691245055\n",
      "i:  80  Error:  0.07757747144659712\n",
      "i:  81  Error:  0.07330746534041117\n",
      "i:  82  Error:  0.06927329582865091\n",
      "i:  83  Error:  0.06546140523136829\n",
      "i:  84  Error:  0.06185891658422893\n",
      "i:  85  Error:  0.058454868988719745\n",
      "i:  86  Error:  0.05523838053943904\n",
      "i:  87  Error:  0.05219915076263189\n",
      "i:  88  Error:  0.04932697543335292\n",
      "i:  89  Error:  0.04661282467844544\n",
      "i:  90  Error:  0.044048166873125474\n",
      "i:  91  Error:  0.04162481906080036\n",
      "i:  92  Error:  0.039334505776576205\n",
      "i:  93  Error:  0.03717024721512588\n",
      "i:  94  Error:  0.0351253925839088\n",
      "i:  95  Error:  0.03319316546709803\n",
      "i:  96  Error:  0.03136735009093251\n",
      "i:  97  Error:  0.02964192407370002\n",
      "i:  98  Error:  0.02801138250286226\n",
      "i:  99  Error:  0.026470664604451646\n"
     ]
    }
   ],
   "source": [
    "final_w1,final_w2,final_w3 = gradient_descent(_x,_y,_z, my_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82ea813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual values for are _w1,_w2,_w3 1 1 1\n",
      "after gradient descent we are getting values for _w1,_w2,_w3 0.9747136046238957 1.010417928333019 0.967631435962175\n"
     ]
    }
   ],
   "source": [
    "print('Actual values for are _w1,_w2,_w3',_w1,_w2,_w3)\n",
    "print('after gradient descent we are getting values for _w1,_w2,_w3',final_w1,final_w2,final_w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07bd5378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are almost reaching near to the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e954aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
